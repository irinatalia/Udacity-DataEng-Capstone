{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData\n",
    "from pyspark.sql.functions import *\n",
    "from uscity_state_codes import us_state_abbrev, state_code_udf, abbrev_state, code_state_udf,code_state_udf,city_codes, city_code_udf\n",
    "from country_codes import country_udf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Create Spark session with SAS7BDAT jar\n",
    "spark = SparkSession\\\n",
    ".builder \\\n",
    ".config(\"spark.jars.packages\",\"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    ".enableHiveSupport().getOrCreate()\n",
    "\n",
    "#Build SQL context object\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "In this project, we will aggregate I94 immigration data by destination city to form our first dimension table. The scond dimenstion table will aggregate city temperature data by city. The two datasets will be joined on destination city to form the fact table. The final database is optimized to query on immigration events to determine if temperature affects the selection of destination cities. Spark will be used to process the data.\n",
    "\n",
    "#### Describe and Gather Data \n",
    "Describe the data sets you're using. Where did it come from? What type of information is included? \n",
    " **I94 Immigration Data:** comes from the [US National Tourism and Trade Office](https://travel.trade.gov/research/reports/i94/historical/2016.html) and includes details on incoming immigrants and their ports of entry.\n",
    "\n",
    "**World Temperature Data:** comes from [kaggle](https://www.kaggle.com/berkeleyearth/climate-change-earth-surface-temperature-data) and includes data on temperature changes in the U.S. since 1850."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Read in the data here\n",
    "# Read April 2016 I94 immigration data into Pandas for exploration\n",
    "df_spark =spark.read.format('com.github.saurfang.sas.spark').load('../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')\n",
    "\n",
    "temperatureData=spark.read.format(\"csv\").option(\"header\", \"true\").load(\"GlobalLandTemperaturesByCity.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Immigration Data by City with Origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# remove nulls then convert i94res codes to country of origin and filter out NULLS and run country_udf function to show state names\n",
    "# country_udf, abbrev_state_udf and city_code_udf were created with data from i94 SAS labels Descriptions file.\n",
    "i94_data=df_spark.filter(df_spark.i94addr.isNotNull())\\\n",
    ".filter(df_spark.i94res.isNotNull())\\\n",
    ".withColumn(\"i94yr\",col(\"i94yr\").cast(\"integer\"))\\\n",
    ".withColumn(\"i94mon\",col(\"i94mon\").cast(\"integer\"))\\\n",
    ".filter(col(\"i94addr\").isin(list(abbrev_state.keys())))\\\n",
    ".filter(col(\"i94port\").isin(list(city_codes.keys())))\\\n",
    ".withColumn(\"city_port_name\",city_code_udf(df_spark[\"i94port\"]))\\\n",
    ".withColumn(\"dest_state_name\",code_state_udf(df_spark[\"i94addr\"]))\\\n",
    ".withColumn(\"origin_country\",country_udf(df_spark[\"i94res\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_I94_Data=i94_data.select(\"cicid\",col(\"i94yr\").alias(\"year\"),col(\"i94mon\").alias(\"month\"),\\\n",
    "                            \"dest_state_name\",\"city_port_name\", \"origin_country\",\"visatype\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+---------------+----------------------+--------------+--------+\n",
      "|cicid|year|month|dest_state_name|city_port_name        |origin_country|visatype|\n",
      "+-----+----+-----+---------------+----------------------+--------------+--------+\n",
      "|7.0  |2016|4    |Alabama        |ATLANTA               |SOUTH KOREA   |F1      |\n",
      "|15.0 |2016|4    |Michigan       |WASHINGTON DC         |ALBANIA       |B2      |\n",
      "|16.0 |2016|4    |Massachusetts  |NEW YORK              |ALBANIA       |B2      |\n",
      "|17.0 |2016|4    |Massachusetts  |NEW YORK              |ALBANIA       |B2      |\n",
      "|18.0 |2016|4    |Michigan       |NEW YORK              |ALBANIA       |B1      |\n",
      "+-----+----+-----+---------------+----------------------+--------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_I94_Data.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Temperature Data by City "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Performing cleaning tasks here\n",
    "\n",
    "#filter the world Temperature Data for only the U.S. and only == 2013 and drop duplicates and convert celcius temp to f\n",
    "usTemperatures=temperatureData.filter(temperatureData[\"country\"]==\"United States\")\\\n",
    ".filter(year(temperatureData[\"dt\"])==2013)\\\n",
    ".withColumn(\"year\",year(temperatureData[\"dt\"]))\\\n",
    ".withColumn(\"month\",month(temperatureData[\"dt\"]))\\\n",
    ".withColumn(\"avg_temp_fahrenheit\",temperatureData[\"AverageTemperature\"]*9/5+32)\\\n",
    ".withColumn(\"city\", temperatureData[\"City\"])\n",
    "#.filter(temperatureData[\"AverageTemperature\"] != 'NaN')\\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "new_Temperatures=usTemperatures.select(\"year\",\"month\",round(col(\"AverageTemperature\"),1).alias(\"avg_temp_celcius\"),\\\n",
    "                                       round(col(\"avg_temp_fahrenheit\"),1).alias(\"avg_temp_fahrenheit\"),\n",
    "                                       upper(col(\"city\")).alias(\"city\"),\"country\").dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----------------+-------------------+-----------+-------------+\n",
      "|year|month|avg_temp_celcius|avg_temp_fahrenheit|       City|      Country|\n",
      "+----+-----+----------------+-------------------+-----------+-------------+\n",
      "|2013|    3|             0.7|               33.3|     AURORA|United States|\n",
      "|2013|    1|             1.0|               33.8|   BELLEVUE|United States|\n",
      "|2013|    7|            24.7|               76.5|CHATTANOOGA|United States|\n",
      "|2013|    9|            22.9|               73.3| CHESAPEAKE|United States|\n",
      "|2013|    7|            25.1|               77.2|   EL MONTE|United States|\n",
      "+----+-----+----------------+-------------------+-----------+-------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_Temperatures.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "Map out the conceptual data model and explain why you chose that model\n",
    "\n",
    "### Star Schema\n",
    "\n",
    "####    Dimension Tables\n",
    "    * immigration_table\n",
    "        year, month, dest_state_name, city_port_name, origin_country, visatype\n",
    "    * city_temperature_table\n",
    "        year, month, avg_temp_celcius, avg_temp_fahrenheit, city, country\n",
    "####    Fact Table\n",
    "    * immigration_to_cities\n",
    "        year, immig_month, immig_origin, to_immig_city, to_immig_city_count, avg_temp_fahrenheit\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "1. Dimension tables will be created from cleansed data.\n",
    "2. Fact table is created as a SQL query with joins to dimension tables.\n",
    "3. Fact table is converted back to a spark dataframe.\n",
    "4. Fact table is written as final parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Create Dimension tables\n",
    "new_I94_Data.createOrReplaceTempView(\"immigration\")\n",
    "new_Temperatures.createOrReplaceTempView(\"temperature\")\n",
    "\n",
    "#allow unlimited time for SQL joins and parquet writes.\n",
    "sqlContext.setConf(\"spark.sql.autoBroadcastJoinThreshold\", \"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# This query will build the fact table by joining to the dimension tables above.\n",
    "immigration_to_cities=spark.sql(\"\"\"SELECT \n",
    "                                    m.year,\n",
    "                                    m.month AS immig_month,\n",
    "                                    m.origin_country AS immig_origin,\n",
    "                                    m.dest_state_name AS to_immig_city,\n",
    "                                    COUNT(m.city_port_name) AS to_immig_city_count,\n",
    "                                    t.avg_temp_fahrenheit                                   \n",
    "                                    \n",
    "                                    FROM immigration m JOIN temperature t ON m.city_port_name=t.City AND m.month=t.month\n",
    "                                   \n",
    "                                    GROUP BY m.year,m.month, m.origin_country,\\\n",
    "                                    m.dest_state_name,m.city_port_name,t.avg_temp_fahrenheit                                    \n",
    "                                    \n",
    "                                    ORDER BY m.origin_country,m.city_port_name                                    \n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+------------+--------------+-------------------+-------------------+\n",
      "|year|immig_month|immig_origin| to_immig_city|to_immig_city_count|avg_temp_fahrenheit|\n",
      "+----+-----------+------------+--------------+-------------------+-------------------+\n",
      "|2016|          4|  ARGENTINA |     Tennessee|                  2|               59.7|\n",
      "|2016|          4|  ARGENTINA |      New York|                 16|               59.7|\n",
      "|2016|          4|  ARGENTINA | Massachusetts|                  2|               59.7|\n",
      "|2016|          4|  ARGENTINA |    Washington|                  3|               59.7|\n",
      "|2016|          4|  ARGENTINA |South Carolina|                  1|               59.7|\n",
      "+----+-----------+------------+--------------+-------------------+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "immigration_to_cities.toDF('year', 'immig_month', 'immig_origin', 'to_immig_city', \\\n",
    "          'to_immig_city_count', 'avg_temp_fahrenheit').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Write fact table to parquet\n",
    "immigration_to_cities.write.parquet(\"immigration_to_cities\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data quality check passed for immigration table with 2783521 records\n",
      "Data quality check passed for temperature table with 2313 records\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform quality checks here\n",
    "\n",
    "def quality_check(df, description):\n",
    "    '''\n",
    "    Input: Spark dataframe, description of Spark datafram\n",
    "    \n",
    "    Output: Print outcome of data quality check\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    result = df.count()\n",
    "    if result == 0:\n",
    "        print(\"Data quality check failed for {} with zero records\".format(description))\n",
    "    else:\n",
    "        print(\"Data quality check passed for {} with {} records\".format(description, result))\n",
    "    return 0\n",
    "\n",
    "# Perform data quality check\n",
    "quality_check(new_I94_Data, \"immigration table\")\n",
    "quality_check(new_Temperatures, \"temperature table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "##### The first dimension table will contain events from the I94 immigration data:\n",
    "year, month, dest_state_name, city_port_name, origin_country, visatype\n",
    "* year: integer (nullable = true)-Year of Immigration\n",
    "* month: integer (nullable = true)-Month of Immigration\n",
    "* dest_state_name: string (nullable = true)-State Name\n",
    "* city_port_name: string (nullable = true)-City Port Name\n",
    "* origin_country: string (nullable = true)-Country of Origin\n",
    "* visatype: string (nullable = true)-Type of visa\n",
    "\n",
    "##### The second dimension table will contain city temperature data. The columns below will be extracted from the temperature dataframe:\n",
    "\n",
    "* year: integer (nullable = true)-Temperature Year\n",
    "* month: integer (nullable = true)-Temerpature Month\n",
    "* avg_temp_celcius: double (nullable = true)-Avg Temperature in Celcius per State\n",
    "* avg_temp_fahrenheit: double (nullable = true)-Avg Temperatrue in Fahrenheit\n",
    "* city: string (nullable = true)-State Name\n",
    "* country: string (nullable = true)-United States\n",
    "\n",
    "##### The fact table will contain information from the I94 immigration data joined with the city temperature data on city name:\n",
    "\n",
    "* year: integer (nullable = true)-Year from immigration table\n",
    "* immig_month: integer (nullable = true)-Month from immigration table\n",
    "* immig_origin: string (nullable = true)-Country of Origin from immigration table\n",
    "* to_immig_city: string (nullable = true)-City immigrated to from immigration table\n",
    "* to_immig_city_count: long (nullable = false)-Total count of people immigrated per city from immigration table\n",
    "* avg_temp_fahrenheit: double (nullable = true)-Avg temperature per state from Temperature table\n",
    " \n",
    "AverageTemperature = average temperature of destination city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Complete Project Write Up\n",
    "* Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "* Propose how often the data should be updated and why.\n",
    "* Write a description of how you would approach the problem differently under the following scenarios:\n",
    " * The data was increased by 100x.\n",
    " * The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    " * The database needed to be accessed by 100+ people."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "1. For this project I used Apache Spark to read, transform, and create data outputs for further analysis. The reason for this was due to the small amount of data and the speed of Spark.\n",
    "\n",
    "2. The data should be updated quarterly. This gives the most up-to-date data for government and organizations.\n",
    "\n",
    "3. Under the following scenarios, I would approach the problem differently:\n",
    "\n",
    "* If the data was increased by 100x, I would use Apache Hadoop to create a distributed processing system for faster processing.\n",
    "* If the data needs to populate a dashboard daily to meet an SLA then I would use a scheduling tool such as Airflow to run the ETL pipeline overnight.\n",
    "* If the data needs to be accessed by 100+ people, I would use a web app running on Amazon AWS for increased capacity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
